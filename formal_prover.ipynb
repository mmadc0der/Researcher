{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae886d0",
   "metadata": {},
   "source": [
    "# **Formal Prover firs overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1d1f9",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b5a25",
   "metadata": {},
   "source": [
    "## Lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20a49624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "from typing import Tuple, NamedTuple, Any\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "class TokenType(Enum):\n",
    "    VAR = auto()        # Variable (e.g., P, Q)\n",
    "    AND = auto()        # Logical AND\n",
    "    OR = auto()         # Logical OR\n",
    "    NOT = auto()        # Logical NOT\n",
    "    IMPLIES = auto()    # Logical IMPLIES (e.g., ->)\n",
    "    IFF = auto()        # Logical IFF (e.g., <->, biconditional)\n",
    "    LPAREN = auto()     # (\n",
    "    RPAREN = auto()     # )\n",
    "    EOF = auto()        # End of formula (End of File/Input)\n",
    "\n",
    "class Token(NamedTuple):\n",
    "    type: TokenType\n",
    "    value: Any = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f2ce59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Optional\n",
    "import re\n",
    "\n",
    "class Lexer:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "        # Define token patterns: longer ones first for operators like \"->\" vs \"<->\"\n",
    "        # For simplicity, we'll use keywords like AND, OR, IMPLIES, IFF\n",
    "        # and single characters for parentheses.\n",
    "        token_specification = [\n",
    "            ('VAR',     r'[A-Za-z_][A-Za-z0-9_]*'), # Variables (must not be keywords)\n",
    "            ('IMPLIES', r'IMPLIES'),\n",
    "            ('IFF',     r'IFF'),\n",
    "            ('AND',     r'AND'),\n",
    "            ('OR',      r'OR'),\n",
    "            ('NOT',     r'NOT'),\n",
    "            ('LPAREN',  r'\\('),\n",
    "            ('RPAREN',  r'\\)'),\n",
    "            ('SKIP',    r'[ \\t]+'),   # Skip spaces and tabs\n",
    "            ('MISMATCH',r'.'),         # Any other character\n",
    "        ]\n",
    "        # Keywords must be checked after VAR pattern if VAR can match keywords\n",
    "        self.keywords = {\n",
    "            \"AND\": TokenType.AND,\n",
    "            \"OR\": TokenType.OR,\n",
    "            \"NOT\": TokenType.NOT,\n",
    "            \"IMPLIES\": TokenType.IMPLIES,\n",
    "            \"IFF\": TokenType.IFF,\n",
    "        }\n",
    "        \n",
    "        tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)\n",
    "        self.tokens: List[Token] = []\n",
    "        for mo in re.finditer(tok_regex, self.text):\n",
    "            kind = mo.lastgroup\n",
    "            value = mo.group()\n",
    "\n",
    "            if kind == 'VAR':\n",
    "                if value.upper() in self.keywords: # Check if it's a keyword\n",
    "                    self.tokens.append(Token(self.keywords[value.upper()], value.upper()))\n",
    "                else:\n",
    "                    self.tokens.append(Token(TokenType.VAR, value))\n",
    "            elif kind in ['AND', 'OR', 'NOT', 'IMPLIES', 'IFF']: # Direct keyword match\n",
    "                 self.tokens.append(Token(self.keywords[value.upper()], value.upper()))\n",
    "            elif kind == 'LPAREN':\n",
    "                self.tokens.append(Token(TokenType.LPAREN, value))\n",
    "            elif kind == 'RPAREN':\n",
    "                self.tokens.append(Token(TokenType.RPAREN, value))\n",
    "            elif kind == 'SKIP':\n",
    "                continue\n",
    "            elif kind == 'MISMATCH':\n",
    "                raise ValueError(f\"Lexer error: Unexpected character '{value}'\")\n",
    "        self.tokens.append(Token(TokenType.EOF))\n",
    "        self.token_idx = 0\n",
    "\n",
    "    def get_next_token(self) -> Token:\n",
    "        if self.token_idx < len(self.tokens):\n",
    "            token = self.tokens[self.token_idx]\n",
    "            self.token_idx += 1\n",
    "            return token\n",
    "        # Should ideally not be reached if EOF is always last and parser checks for it\n",
    "        return Token(TokenType.EOF) \n",
    "\n",
    "    def peek_token(self) -> Token:\n",
    "        if self.token_idx < len(self.tokens):\n",
    "            return self.tokens[self.token_idx]\n",
    "        return Token(TokenType.EOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f956165",
   "metadata": {},
   "source": [
    "## Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfa3cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTNode:\n",
    "    \"\"\"Base class for AST nodes.\"\"\"\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(...)\"\n",
    "\n",
    "class Variable(ASTNode):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "    def __repr__(self):\n",
    "        return f\"Variable(name='{self.name}')\"\n",
    "\n",
    "class UnaryOp(ASTNode):\n",
    "    def __init__(self, op: Token, operand: ASTNode):\n",
    "        self.op_token = op # The token for the operator (e.g., NOT)\n",
    "        self.op = op.type # The TokenType\n",
    "        self.operand = operand\n",
    "    def __repr__(self):\n",
    "        return f\"UnaryOp(op='{self.op_token.value}', operand={self.operand})\"\n",
    "\n",
    "class BinaryOp(ASTNode):\n",
    "    def __init__(self, left: ASTNode, op: Token, right: ASTNode):\n",
    "        self.left = left\n",
    "        self.op_token = op # The token for the operator (e.g., AND, OR)\n",
    "        self.op = op.type  # The TokenType\n",
    "        self.right = right\n",
    "    def __repr__(self):\n",
    "        return f\"BinaryOp(left={self.left}, op='{self.op_token.value}', right={self.right})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1a08a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, lexer: Lexer):\n",
    "        self.lexer = lexer\n",
    "        self.current_token: Token = self.lexer.get_next_token()\n",
    "\n",
    "    def _eat(self, token_type: TokenType):\n",
    "        if self.current_token.type == token_type:\n",
    "            self.current_token = self.lexer.get_next_token()\n",
    "        else:\n",
    "            raise SyntaxError(\n",
    "                f\"Parser error: Expected token {token_type}, \"\n",
    "                f\"but got {self.current_token.type} (value: '{self.current_token.value}')\"\n",
    "            )\n",
    "\n",
    "    def _atom(self) -> ASTNode:\n",
    "        \"\"\"atom : VAR | LPAREN expression RPAREN\"\"\"\n",
    "        token = self.current_token\n",
    "        if token.type == TokenType.VAR:\n",
    "            self._eat(TokenType.VAR)\n",
    "            return Variable(token.value)\n",
    "        elif token.type == TokenType.LPAREN:\n",
    "            self._eat(TokenType.LPAREN)\n",
    "            node = self._expression()\n",
    "            self._eat(TokenType.RPAREN)\n",
    "            return node\n",
    "        else:\n",
    "            raise SyntaxError(f\"Parser error (_atom): Expected VAR or LPAREN, got {token.type}\")\n",
    "\n",
    "    def _negation(self) -> ASTNode:\n",
    "        \"\"\"negation : NOT negation | atom\"\"\"\n",
    "        token = self.current_token\n",
    "        if token.type == TokenType.NOT:\n",
    "            self._eat(TokenType.NOT)\n",
    "            # Note: NOT applies to the result of the next _negation call,\n",
    "            # allowing for NOT NOT P or NOT (P AND Q)\n",
    "            node = self._negation() \n",
    "            return UnaryOp(token, node)\n",
    "        else:\n",
    "            return self._atom()\n",
    "\n",
    "    def _conjunction(self) -> ASTNode:\n",
    "        \"\"\"conjunction : negation (AND negation)*\"\"\"\n",
    "        node = self._negation()\n",
    "        while self.current_token.type == TokenType.AND:\n",
    "            op_token = self.current_token\n",
    "            self._eat(TokenType.AND)\n",
    "            node = BinaryOp(node, op_token, self._negation())\n",
    "        return node\n",
    "\n",
    "    def _disjunction(self) -> ASTNode:\n",
    "        \"\"\"disjunction : conjunction (OR conjunction)*\"\"\"\n",
    "        node = self._conjunction()\n",
    "        while self.current_token.type == TokenType.OR:\n",
    "            op_token = self.current_token\n",
    "            self._eat(TokenType.OR)\n",
    "            node = BinaryOp(node, op_token, self._conjunction())\n",
    "        return node\n",
    "\n",
    "    def _implication(self) -> ASTNode:\n",
    "        \"\"\"implication : disjunction (IMPLIES disjunction)*\"\"\"\n",
    "        # This implements left-associative IMPLIES.\n",
    "        # P IMPLIES Q IMPLIES R  ->  ((P IMPLIES Q) IMPLIES R)\n",
    "        # Often, IMPLIES is right-associative: P IMPLIES (Q IMPLIES R)\n",
    "        # For right-associativity, the rule would be:\n",
    "        # implication : disjunction (IMPLIES implication)?\n",
    "        node = self._disjunction()\n",
    "        while self.current_token.type == TokenType.IMPLIES:\n",
    "            op_token = self.current_token\n",
    "            self._eat(TokenType.IMPLIES)\n",
    "            node = BinaryOp(node, op_token, self._disjunction()) # For left-associativity\n",
    "            # For right-associativity: node = BinaryOp(node, op_token, self._implication())\n",
    "        return node\n",
    "        \n",
    "    def _expression(self) -> ASTNode:\n",
    "        \"\"\"expression : implication (IFF implication)*\"\"\"\n",
    "        # IFF is also often non-associative or has specific rules.\n",
    "        # Here, implemented as left-associative for simplicity.\n",
    "        node = self._implication()\n",
    "        while self.current_token.type == TokenType.IFF:\n",
    "            op_token = self.current_token\n",
    "            self._eat(TokenType.IFF)\n",
    "            node = BinaryOp(node, op_token, self._implication())\n",
    "        return node\n",
    "\n",
    "    def parse(self) -> ASTNode:\n",
    "        \"\"\"Parses the full input string and returns the AST root.\"\"\"\n",
    "        ast_root = self._expression()\n",
    "        if self.current_token.type != TokenType.EOF:\n",
    "            raise SyntaxError(\n",
    "                f\"Parser error: Unexpected token {self.current_token} \"\n",
    "                f\"(value: '{self.current_token.value}') at end of input. Expected EOF.\"\n",
    "            )\n",
    "        return ast_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb9168f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula1_str = \"P AND (Q OR NOT R)\"\n",
    "formula2_str = \"P IMPLIES (Q AND P)\"\n",
    "formula_tautology_str = \"A OR NOT A\"  # Это тавтология\n",
    "formula_contradiction_str = \"A AND NOT A\" # Это противоречие (не тавтология)\n",
    "formula_complex_tautology = \"(P IMPLIES Q) IFF (NOT P OR Q)\" # Тавтология (определение импликации)\n",
    "malformed_formula = \"P AND (Q OR\" # Синтаксически некорректная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f10e6c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryOp(left=Variable(name='P'), op='AND', right=BinaryOp(left=Variable(name='Q'), op='OR', right=UnaryOp(op='NOT', operand=Variable(name='R'))))\n"
     ]
    }
   ],
   "source": [
    "formula = \"P AND (Q OR NOT R)\"\n",
    "lexer = Lexer(formula)\n",
    "parser = Parser(lexer)\n",
    "ast = parser.parse()\n",
    "print(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d4cf3",
   "metadata": {},
   "source": [
    "## Logics Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0614ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicsEngine:\n",
    "    def __init__(self, axioms: Set[str]):\n",
    "        self.known_theorems_str: Set[str] = set()\n",
    "        for axiom_str in axioms:\n",
    "            try:\n",
    "                ast = self._parse_formula_string(axiom_str)\n",
    "                canonical_str = self._ast_to_canonical_string(ast)\n",
    "                if not canonical_str:\n",
    "                    raise ValueError(\"Received empty canonical string for an axiom.\")\n",
    "                self.known_theorems_str.add(canonical_str)\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                raise ValueError(f\"Initialization error: Invalid axiom '{axiom_str}': {e}\")\n",
    "\n",
    "    def _parse_formula_string(self, formula_str: str) -> ASTNode:\n",
    "        if not formula_str.strip():\n",
    "            raise ValueError(\"Cannot parse an empty formula string.\")\n",
    "        lexer = Lexer(formula_str)\n",
    "        parser = Parser(lexer)\n",
    "        return parser.parse()\n",
    "\n",
    "    def _ast_to_canonical_string_recursive(self, node: ASTNode, is_operand: bool) -> str:\n",
    "        if isinstance(node, Variable):\n",
    "            return node.name\n",
    "        elif isinstance(node, UnaryOp):\n",
    "            operand_s = self._ast_to_canonical_string_recursive(node.operand, True)\n",
    "            # Ensure NOT has its operand potentially bracketed, and the whole NOT expression is bracketed if it's an operand\n",
    "            res_str = f\"{node.op_token.value} {operand_s}\"\n",
    "            return f\"({res_str})\" if is_operand else res_str\n",
    "        elif isinstance(node, BinaryOp):\n",
    "            left_s = self._ast_to_canonical_string_recursive(node.left, True)\n",
    "            right_s = self._ast_to_canonical_string_recursive(node.right, True)\n",
    "            expr_str = f\"{left_s} {node.op_token.value} {right_s}\"\n",
    "            return f\"({expr_str})\" if is_operand else expr_str\n",
    "        else:\n",
    "            raise TypeError(f\"Unknown AST node type for string conversion: {type(node)}\")\n",
    "\n",
    "    def _ast_to_canonical_string(self, node: ASTNode) -> str:\n",
    "        return self._ast_to_canonical_string_recursive(node, False)\n",
    "\n",
    "    def add_theorem_ast(self, formula_ast: ASTNode) -> bool:\n",
    "        canonical_str = self._ast_to_canonical_string(formula_ast)\n",
    "        if canonical_str not in self.known_theorems_str:\n",
    "            self.known_theorems_str.add(canonical_str)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _add_and_return_canonical(self, formula_ast: ASTNode) -> str:\n",
    "        \"\"\"Helper to add theorem and return its canonical string.\"\"\"\n",
    "        self.add_theorem_ast(formula_ast)\n",
    "        return self._ast_to_canonical_string(formula_ast)\n",
    "\n",
    "    def is_known(self, formula_str: str) -> bool:\n",
    "        try:\n",
    "            ast = self._parse_formula_string(formula_str)\n",
    "            canonical_str = self._ast_to_canonical_string(ast)\n",
    "            return canonical_str in self.known_theorems_str\n",
    "        except (ValueError, SyntaxError):\n",
    "            return False\n",
    "\n",
    "    def get_known_formulas(self) -> Set[str]:\n",
    "        return self.known_theorems_str.copy()\n",
    "\n",
    "    # --- Inference Rules ---\n",
    "\n",
    "    def apply_modus_ponens(self, premise_p_str: str, premise_p_implies_q_str: str) -> Optional[str]:\n",
    "        if not (self.is_known(premise_p_str) and self.is_known(premise_p_implies_q_str)):\n",
    "            return None\n",
    "        try:\n",
    "            ast_p = self._parse_formula_string(premise_p_str)\n",
    "            ast_p_implies_q = self._parse_formula_string(premise_p_implies_q_str)\n",
    "        except (ValueError, SyntaxError): return None\n",
    "\n",
    "        # Case 1: ast_p_implies_q is (P IMPLIES Q), ast_p is P\n",
    "        if isinstance(ast_p_implies_q, BinaryOp) and ast_p_implies_q.op_token.type == TokenType.IMPLIES:\n",
    "            antecedent = ast_p_implies_q.left\n",
    "            consequent = ast_p_implies_q.right\n",
    "            if self._ast_to_canonical_string(antecedent) == self._ast_to_canonical_string(ast_p):\n",
    "                return self._add_and_return_canonical(consequent)\n",
    "        \n",
    "        # Case 2: ast_p is (P IMPLIES Q), ast_p_implies_q is P (swapped arguments)\n",
    "        if isinstance(ast_p, BinaryOp) and ast_p.op_token.type == TokenType.IMPLIES:\n",
    "            antecedent = ast_p.left\n",
    "            consequent = ast_p.right\n",
    "            if self._ast_to_canonical_string(antecedent) == self._ast_to_canonical_string(ast_p_implies_q):\n",
    "                 return self._add_and_return_canonical(consequent)\n",
    "        return None\n",
    "\n",
    "    def apply_modus_tollens(self, premise_p_implies_q_str: str, premise_not_q_str: str) -> Optional[str]:\n",
    "        \"\"\"Applies Modus Tollens: (P IMPLIES Q), (NOT Q) |- (NOT P)\"\"\"\n",
    "        if not (self.is_known(premise_p_implies_q_str) and self.is_known(premise_not_q_str)):\n",
    "            return None\n",
    "        try:\n",
    "            ast_p_implies_q = self._parse_formula_string(premise_p_implies_q_str)\n",
    "            ast_not_q = self._parse_formula_string(premise_not_q_str)\n",
    "        except (ValueError, SyntaxError): return None\n",
    "\n",
    "        # Check structure of (P IMPLIES Q)\n",
    "        if not (isinstance(ast_p_implies_q, BinaryOp) and ast_p_implies_q.op_token.type == TokenType.IMPLIES):\n",
    "            return None\n",
    "        \n",
    "        p_ast = ast_p_implies_q.left  # This is P\n",
    "        q_ast = ast_p_implies_q.right # This is Q\n",
    "\n",
    "        # Check structure of (NOT Q)\n",
    "        if not (isinstance(ast_not_q, UnaryOp) and ast_not_q.op_token.type == TokenType.NOT):\n",
    "            return None\n",
    "        \n",
    "        q_from_not_q_ast = ast_not_q.operand # This is Q from (NOT Q)\n",
    "\n",
    "        # Verify that Q from (P IMPLIES Q) is the same as Q from (NOT Q)\n",
    "        if self._ast_to_canonical_string(q_ast) == self._ast_to_canonical_string(q_from_not_q_ast):\n",
    "            # Construct (NOT P)\n",
    "            not_p_ast = UnaryOp(Token(TokenType.NOT, \"NOT\"), p_ast)\n",
    "            return self._add_and_return_canonical(not_p_ast)\n",
    "        return None\n",
    "\n",
    "    def apply_and_introduction(self, premise_p_str: str, premise_q_str: str) -> Optional[str]:\n",
    "        \"\"\"Applies And Introduction: P, Q |- (P AND Q)\"\"\"\n",
    "        if not (self.is_known(premise_p_str) and self.is_known(premise_q_str)):\n",
    "            return None\n",
    "        try:\n",
    "            ast_p = self._parse_formula_string(premise_p_str)\n",
    "            ast_q = self._parse_formula_string(premise_q_str)\n",
    "        except (ValueError, SyntaxError): return None\n",
    "\n",
    "        # Construct (P AND Q)\n",
    "        p_and_q_ast = BinaryOp(ast_p, Token(TokenType.AND, \"AND\"), ast_q)\n",
    "        return self._add_and_return_canonical(p_and_q_ast)\n",
    "\n",
    "    def apply_and_elimination1(self, premise_p_and_q_str: str) -> Optional[str]:\n",
    "        \"\"\"Applies And Elimination 1: (P AND Q) |- P\"\"\"\n",
    "        if not self.is_known(premise_p_and_q_str): return None\n",
    "        try:\n",
    "            ast_p_and_q = self._parse_formula_string(premise_p_and_q_str)\n",
    "        except (ValueError, SyntaxError): return None\n",
    "\n",
    "        if isinstance(ast_p_and_q, BinaryOp) and ast_p_and_q.op_token.type == TokenType.AND:\n",
    "            p_ast = ast_p_and_q.left\n",
    "            return self._add_and_return_canonical(p_ast)\n",
    "        return None\n",
    "\n",
    "    def apply_and_elimination2(self, premise_p_and_q_str: str) -> Optional[str]:\n",
    "        \"\"\"Applies And Elimination 2: (P AND Q) |- Q\"\"\"\n",
    "        if not self.is_known(premise_p_and_q_str): return None\n",
    "        try:\n",
    "            ast_p_and_q = self._parse_formula_string(premise_p_and_q_str)\n",
    "        except (ValueError, SyntaxError): return None\n",
    "\n",
    "        if isinstance(ast_p_and_q, BinaryOp) and ast_p_and_q.op_token.type == TokenType.AND:\n",
    "            q_ast = ast_p_and_q.right\n",
    "            return self._add_and_return_canonical(q_ast)\n",
    "        return None\n",
    "\n",
    "    def apply_double_negation_elimination(self, premise_not_not_p_str: str) -> Optional[str]:\n",
    "        \"\"\"Applies Double Negation Elimination: (NOT (NOT P)) |- P\"\"\"\n",
    "        if not self.is_known(premise_not_not_p_str): return None\n",
    "        try:\n",
    "            ast_not_not_p = self._parse_formula_string(premise_not_not_p_str)\n",
    "        except (ValueError, SyntaxError): return None\n",
    "\n",
    "        if isinstance(ast_not_not_p, UnaryOp) and ast_not_not_p.op_token.type == TokenType.NOT:\n",
    "            inner_operand = ast_not_not_p.operand\n",
    "            if isinstance(inner_operand, UnaryOp) and inner_operand.op_token.type == TokenType.NOT:\n",
    "                p_ast = inner_operand.operand\n",
    "                return self._add_and_return_canonical(p_ast)\n",
    "        return None\n",
    "\n",
    "    def apply_double_negation_introduction(self, p_str: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Applies Double Negation Introduction: P |- NOT (NOT P)\n",
    "        \"\"\"\n",
    "        if not self.is_known(p_str):\n",
    "            return None\n",
    "        try:\n",
    "            p_ast = self._parse_formula_string(p_str)\n",
    "            \n",
    "            # NOT P\n",
    "            not_p_ast = UnaryOp(Token(TokenType.NOT, \"NOT\"), p_ast)\n",
    "            # NOT (NOT P)\n",
    "            not_not_p_ast = UnaryOp(Token(TokenType.NOT, \"NOT\"), not_p_ast)\n",
    "            \n",
    "            return self._add_and_return_canonical(not_not_p_ast)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "\n",
    "    def _recursive_or_simplification(self, node: ASTNode, part_canonical_str: str) -> Tuple[ASTNode, bool]:\n",
    "        \"\"\"\n",
    "        Recursively traverses the AST. If it finds a disjunction (OR) where one of\n",
    "        the operands matches part_canonical_str, it replaces the entire disjunction\n",
    "        with that operand.\n",
    "\n",
    "        Args:\n",
    "            node: The current ASTNode to process.\n",
    "            part_canonical_str: The canonical string of the part known to be true.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - The new (potentially simplified) ASTNode.\n",
    "                - A boolean flag indicating whether a change was made in this subtree.\n",
    "        \"\"\"\n",
    "        if isinstance(node, Variable):\n",
    "            return node, False\n",
    "\n",
    "        if isinstance(node, UnaryOp):\n",
    "            new_operand, changed = self._recursive_or_simplification(node.operand, part_canonical_str)\n",
    "            if changed:\n",
    "                return UnaryOp(node.op_token, new_operand), True\n",
    "            return node, False\n",
    "\n",
    "        if isinstance(node, BinaryOp):\n",
    "            # First, check if the current node itself is an OR expression to be simplified.\n",
    "            if node.op == TokenType.OR:\n",
    "                left_canonical = self._ast_to_canonical_string(node.left)\n",
    "                right_canonical = self._ast_to_canonical_string(node.right)\n",
    "\n",
    "                if part_canonical_str == left_canonical:\n",
    "                    # The known part matches the left side of the OR.\n",
    "                    # Replace 'P OR Q' with 'P'.\n",
    "                    return node.left, True \n",
    "                if part_canonical_str == right_canonical:\n",
    "                    # The known part matches the right side of the OR.\n",
    "                    # Replace 'P OR Q' with 'Q'.\n",
    "                    return node.right, True\n",
    "\n",
    "            # If the node itself was not simplified, recursively call for its children.\n",
    "            new_left, left_changed = self._recursive_or_simplification(node.left, part_canonical_str)\n",
    "            new_right, right_changed = self._recursive_or_simplification(node.right, part_canonical_str)\n",
    "\n",
    "            if left_changed or right_changed:\n",
    "                # If any child node changed, create a new BinaryOp node with the updated children.\n",
    "                return BinaryOp(new_left, node.op_token, new_right), True\n",
    "            \n",
    "            # No changes were made in this subtree.\n",
    "            return node, False\n",
    "        \n",
    "        # For other node types (if they appear in the future)\n",
    "        return node, False\n",
    "\n",
    "\n",
    "    def apply_or_elimination(self, part_str: str, complex_formula_str: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Applies a custom version of Or Elimination via recursive substitution.\n",
    "        If we know P and have a formula phi(A OR P), we can infer phi(P).\n",
    "        This simplifies the complex formula by replacing a disjunction with its known part.\n",
    "\n",
    "        Args:\n",
    "            part_str: The part of a disjunction known to be true (e.g., 'P').\n",
    "            complex_formula_str: The complex formula potentially containing the disjunction.\n",
    "        \"\"\"\n",
    "        if not (self.is_known(part_str) and self.is_known(complex_formula_str)):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Get the canonical representation of the part we know.\n",
    "            part_ast = self._parse_formula_string(part_str)\n",
    "            part_canonical = self._ast_to_canonical_string(part_ast)\n",
    "\n",
    "            # Parse the complex formula we want to simplify.\n",
    "            complex_ast = self._parse_formula_string(complex_formula_str)\n",
    "\n",
    "            # Perform the recursive simplification.\n",
    "            new_ast, changed = self._recursive_or_simplification(complex_ast, part_canonical)\n",
    "            \n",
    "            if not changed:\n",
    "                # The known part was not found within a disjunction in the complex formula,\n",
    "                # or no simplification was possible.\n",
    "                return None\n",
    "\n",
    "            return self._add_and_return_canonical(new_ast)\n",
    "\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None\n",
    "\n",
    "    def apply_hypothetical_syllogism(self, premise1_str: str, premise2_str: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Applies Hypothetical Syllogism: (P IMPLIES Q), (Q IMPLIES R) |- (P IMPLIES R)\n",
    "        premise1_str: P IMPLIES Q\n",
    "        premise2_str: Q IMPLIES R\n",
    "        \"\"\"\n",
    "        if not (self.is_known(premise1_str) and self.is_known(premise2_str)):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            ast1 = self._parse_formula_string(premise1_str) # P IMPLIES Q\n",
    "            ast2 = self._parse_formula_string(premise2_str) # Q IMPLIES R\n",
    "\n",
    "            # Check structure of premise1: P IMPLIES Q\n",
    "            if not (isinstance(ast1, BinaryOp) and ast1.op == TokenType.IMPLIES):\n",
    "                return None\n",
    "            p_ast = ast1.left\n",
    "            q1_ast = ast1.right\n",
    "\n",
    "            # Check structure of premise2: Q IMPLIES R\n",
    "            if not (isinstance(ast2, BinaryOp) and ast2.op == TokenType.IMPLIES):\n",
    "                return None\n",
    "            q2_ast = ast2.left\n",
    "            r_ast = ast2.right\n",
    "\n",
    "            # Check if Q from (P IMPLIES Q) is the same as Q from (Q IMPLIES R)\n",
    "            q1_str_canonical = self._ast_to_canonical_string(q1_ast)\n",
    "            q2_str_canonical = self._ast_to_canonical_string(q2_ast)\n",
    "\n",
    "            if q1_str_canonical != q2_str_canonical:\n",
    "                return None\n",
    "                \n",
    "            # Construct P IMPLIES R\n",
    "            p_implies_r_ast = BinaryOp(p_ast, Token(TokenType.IMPLIES, \"IMPLIES\"), r_ast)\n",
    "            \n",
    "            return self._add_and_return_canonical(p_implies_r_ast)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a432a57",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37f011e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np # For observation space if using numerical representations\n",
    "from typing import Set, List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6ef4f",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7f69d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceRule(Enum):\n",
    "    MODUS_PONENS = auto()\n",
    "    MODUS_TOLLENS = auto()\n",
    "    AND_INTRODUCTION = auto()\n",
    "    AND_ELIMINATION1 = auto() # P AND Q |- P\n",
    "    AND_ELIMINATION2 = auto() # P AND Q |- Q\n",
    "    DOUBLE_NEGATION_ELIMINATION = auto()\n",
    "    DOUBLE_NEGATION_INTRODUCTION = auto()\n",
    "    OR_ELIMINATION = auto() # Q, P OR Q |- Q\n",
    "    HYPOTHETICAL_SYLLOGISM = auto() # P IMPLIES Q, Q IMPLIES R |- P IMPLIES R\n",
    "\n",
    "class ProofSearchEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 4}\n",
    "\n",
    "    def __init__(self, axioms: Set[str], goal_formula_str: str,\n",
    "                 rules: List[InferenceRule],\n",
    "                 max_formulas_in_state: int = 20, # Max formulas agent can directly reference by index\n",
    "                 max_formula_id_for_obs: int = 100, # Max ID for observation space\n",
    "                 max_steps: int = 50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial_axioms = axioms.copy()\n",
    "        self.goal_formula_str = goal_formula_str\n",
    "        self.max_formulas_in_actionable_list = max_formulas_in_state\n",
    "        self.max_steps = max_steps\n",
    "        self.available_rules = rules\n",
    "\n",
    "        self.engine = LogicsEngine(self.initial_axioms)\n",
    "        try:\n",
    "            self.goal_ast = self.engine._parse_formula_string(self.goal_formula_str)\n",
    "            self.goal_canonical_str = self.engine._ast_to_canonical_string(self.goal_ast)\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            raise ValueError(f\"Goal formula '{goal_formula_str}' is invalid: {e}\")\n",
    "\n",
    "        # For mapping formula strings to unique integer IDs for the observation vector\n",
    "        self.formula_to_observation_id: Dict[str, int] = {}\n",
    "        self.next_observation_id_counter: int = 1 # 0 is reserved for \"empty/padding\"\n",
    "\n",
    "        # List of formula strings that the agent can select as premises by their index\n",
    "        self.actionable_formulas_list: List[Optional[str]] = [None] * self.max_formulas_in_actionable_list\n",
    "        self.next_actionable_slot_idx: int = 0\n",
    "\n",
    "        self.observation_space = spaces.MultiDiscrete([max_formula_id_for_obs] * self.max_formulas_in_actionable_list)\n",
    "\n",
    "        self._initialize_actionable_formulas_and_vocab()\n",
    "\n",
    "        # Action: (rule_index, index_premise1, index_premise2)\n",
    "        self.action_space = spaces.Tuple((\n",
    "            spaces.Discrete(len(self.available_rules)),\n",
    "            spaces.Discrete(self.max_formulas_in_actionable_list),\n",
    "            spaces.Discrete(self.max_formulas_in_actionable_list)\n",
    "        ))\n",
    "        \n",
    "        self.current_step = 0\n",
    "\n",
    "    def _get_or_assign_observation_id(self, formula_str: str) -> int:\n",
    "        \"\"\"Assigns a new ID if formula_str is new, or returns existing ID.\"\"\"\n",
    "        if formula_str not in self.formula_to_observation_id:\n",
    "            # Simple strategy: if we exceed max_formula_id_for_obs, reuse last ID or a special one.\n",
    "            # This is a limitation; proper vocabulary management is complex.\n",
    "            if self.next_observation_id_counter < self.observation_space.nvec[0]: # Check against max ID for observation\n",
    "                self.formula_to_observation_id[formula_str] = self.next_observation_id_counter\n",
    "                self.next_observation_id_counter += 1\n",
    "            else:\n",
    "                # Out of observation IDs, assign a \"fallback\" ID (e.g., max_id - 1)\n",
    "                # This means new, distinct formulas might map to the same observation ID.\n",
    "                return self.observation_space.nvec[0] - 1 \n",
    "        return self.formula_to_observation_id[formula_str]\n",
    "\n",
    "    def _ensure_formula_in_actionable_list(self, formula_str: str):\n",
    "        \"\"\"Adds formula_str to actionable_formulas_list if not present and space allows.\"\"\"\n",
    "        # Check if already present (by value)\n",
    "        if formula_str in self.actionable_formulas_list:\n",
    "            return\n",
    "\n",
    "        if self.next_actionable_slot_idx < self.max_formulas_in_actionable_list:\n",
    "            self.actionable_formulas_list[self.next_actionable_slot_idx] = formula_str\n",
    "            self.next_actionable_slot_idx += 1\n",
    "        # If no space, the formula is known by the engine but not directly actionable by index.\n",
    "        \n",
    "        # Ensure it has an observation ID\n",
    "        self._get_or_assign_observation_id(formula_str)\n",
    "\n",
    "\n",
    "    def _initialize_actionable_formulas_and_vocab(self):\n",
    "        \"\"\"Called by __init__ and reset.\"\"\"\n",
    "        self.formula_to_observation_id.clear()\n",
    "        self.next_observation_id_counter = 1 \n",
    "        self.actionable_formulas_list = [None] * self.max_formulas_in_actionable_list\n",
    "        self.next_actionable_slot_idx = 0\n",
    "\n",
    "        # Add initial axioms to actionable list and observation vocabulary\n",
    "        for axiom_str in self.initial_axioms:\n",
    "            self._ensure_formula_in_actionable_list(axiom_str)\n",
    "        \n",
    "        # Ensure goal formula is in observation vocabulary (even if not actionable initially)\n",
    "        if self.goal_canonical_str:\n",
    "            self._get_or_assign_observation_id(self.goal_canonical_str)\n",
    "\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        obs = np.zeros(self.max_formulas_in_actionable_list, dtype=int)\n",
    "        for i, formula_str in enumerate(self.actionable_formulas_list):\n",
    "            if formula_str:\n",
    "                obs[i] = self._get_or_assign_observation_id(formula_str)\n",
    "            else:\n",
    "                obs[i] = 0 # 0 for empty slot\n",
    "        return obs\n",
    "\n",
    "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self.engine = LogicsEngine(self.initial_axioms) # Re-init engine\n",
    "        self.current_step = 0\n",
    "        self._initialize_actionable_formulas_and_vocab()\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def step(self, action: Tuple[int, int, int]) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        self.current_step += 1\n",
    "        rule_idx, premise1_idx, premise2_idx = action\n",
    "        \n",
    "        selected_rule = self.available_rules[rule_idx]\n",
    "\n",
    "        reward = -0.1  # Default penalty per step\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        newly_derived_formula_str: Optional[str] = None # Store the string of the derived formula\n",
    "\n",
    "        premise1_str = self.actionable_formulas_list[premise1_idx]\n",
    "        premise2_str = self.actionable_formulas_list[premise2_idx]\n",
    "        \n",
    "        num_known_before_engine = len(self.engine.get_known_formulas())\n",
    "        derived_q_engine_str: Optional[str] = None # String returned by engine method\n",
    "\n",
    "        # Check if premises are valid (not None) before calling engine\n",
    "        can_apply_unary = premise1_str is not None\n",
    "        can_apply_binary = premise1_str is not None and premise2_str is not None\n",
    "\n",
    "        if selected_rule == InferenceRule.MODUS_PONENS:\n",
    "            if can_apply_binary:\n",
    "                derived_q_engine_str = self.engine.apply_modus_ponens(premise1_str, premise2_str)\n",
    "        elif selected_rule == InferenceRule.MODUS_TOLLENS:\n",
    "            if can_apply_binary:\n",
    "                derived_q_engine_str = self.engine.apply_modus_tollens(premise1_str, premise2_str)\n",
    "        elif selected_rule == InferenceRule.AND_INTRODUCTION:\n",
    "            if can_apply_binary:\n",
    "                # Prevent P AND P if P is selected twice for binary op, unless desired\n",
    "                if premise1_str == premise2_str: \n",
    "                    reward = -0.3 # Penalize redundant AND intro\n",
    "                else:\n",
    "                    derived_q_engine_str = self.engine.apply_and_introduction(premise1_str, premise2_str)\n",
    "        elif selected_rule == InferenceRule.AND_ELIMINATION1:\n",
    "            if can_apply_unary:\n",
    "                derived_q_engine_str = self.engine.apply_and_elimination1(premise1_str)\n",
    "        elif selected_rule == InferenceRule.AND_ELIMINATION2:\n",
    "            if can_apply_unary:\n",
    "                derived_q_engine_str = self.engine.apply_and_elimination2(premise1_str)\n",
    "        elif selected_rule == InferenceRule.DOUBLE_NEGATION_ELIMINATION:\n",
    "            if can_apply_unary:\n",
    "                derived_q_engine_str = self.engine.apply_double_negation_elimination(premise1_str)\n",
    "        elif selected_rule == InferenceRule.DOUBLE_NEGATION_INTRODUCTION:\n",
    "            if can_apply_unary: # P |- NOT NOT P\n",
    "                derived_q_engine_str = self.engine.apply_double_negation_introduction(premise1_str)\n",
    "        elif selected_rule == InferenceRule.OR_ELIMINATION: # P OR Q, P |- Q\n",
    "            if can_apply_binary: # premise1_str is P->Q, premise2_str is Q->R\n",
    "                derived_q_engine_str = self.engine.apply_or_elimination(premise1_str, premise2_str)\n",
    "        elif selected_rule == InferenceRule.HYPOTHETICAL_SYLLOGISM: # P IMPLIES Q, Q IMPLIES R |- P IMPLIES R\n",
    "            if can_apply_binary: # premise1_str is P->Q, premise2_str is Q->R\n",
    "                derived_q_engine_str = self.engine.apply_hypothetical_syllogism(premise1_str, premise2_str)\n",
    "        elif (can_apply_unary or can_apply_binary): # Premises were valid, but rule didn't apply (e.g. wrong form)\n",
    "            reward = -0.5\n",
    "        else: \n",
    "            reward = -2.0\n",
    "\n",
    "        if derived_q_engine_str:\n",
    "            newly_derived_formula_str = derived_q_engine_str # Keep this for info\n",
    "            \n",
    "            # Check if the formula was TRULY new to the engine's knowledge base\n",
    "            if len(self.engine.get_known_formulas()) > num_known_before_engine:\n",
    "                reward = 0.8\n",
    "                self._ensure_formula_in_actionable_list(derived_q_engine_str) # Make it actionable\n",
    "            else: # Formula was already known by the engine\n",
    "                reward = -0.2 \n",
    "\n",
    "            if derived_q_engine_str == self.goal_canonical_str:\n",
    "                reward = 64.0\n",
    "                terminated = True\n",
    "        # Penalties for invalid actions or rules that didn't apply\n",
    "        elif not can_apply_unary and selected_rule in [InferenceRule.AND_ELIMINATION1, InferenceRule.AND_ELIMINATION2, InferenceRule.DOUBLE_NEGATION_ELIMINATION, InferenceRule.DOUBLE_NEGATION_INTRODUCTION]:\n",
    "            reward = -1.0 # Tried unary rule on empty slot\n",
    "        elif not can_apply_binary and selected_rule in [InferenceRule.MODUS_PONENS, InferenceRule.MODUS_TOLLENS, InferenceRule.AND_INTRODUCTION, InferenceRule.OR_ELIMINATION, InferenceRule.HYPOTHETICAL_SYLLOGISM]:\n",
    "            reward = -1.0 # Tried binary rule with at least one empty slot\n",
    "\n",
    "        if self.current_step >= self.max_steps:\n",
    "            truncated = True\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        return observation, reward, terminated, truncated, {\n",
    "            \"newly_derived_formula\": newly_derived_formula_str, \n",
    "            \"rule_used\": selected_rule.name,\n",
    "            \"action_indices\": (premise1_idx, premise2_idx)\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            print(f\"--- Step: {self.current_step} ---\")\n",
    "            print(\"Actionable Formulas (agent's view for premise selection):\")\n",
    "            for i, f_str in enumerate(self.actionable_formulas_list):\n",
    "                if f_str:\n",
    "                    obs_id = self.formula_to_observation_id.get(f_str, \"N/A\")\n",
    "                    print(f\"  idx {i}: {f_str} (obs_id: {obs_id})\")\n",
    "                else:\n",
    "                    print(f\"  idx {i}: <Empty>\")\n",
    "            \n",
    "            print(f\"\\nEngine's Known Formulas ({len(self.engine.get_known_formulas())}):\")\n",
    "            # for f_str_known in sorted(list(self.engine.get_known_formulas())):\n",
    "            #     print(f\"  - {f_str_known}\")\n",
    "\n",
    "            print(f\"Goal: {self.goal_canonical_str}\")\n",
    "            if self.engine.is_known(self.goal_canonical_str):\n",
    "                print(\">>> Goal has been reached! <<<\")\n",
    "            print(\"-\" * 20)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24c95c",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc3c38",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f044ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, rule_dim, num_actions, hidden_dim=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Actor heads\n",
    "        self.rule_head = nn.Linear(hidden_dim, rule_dim)\n",
    "        self.action1_head = nn.Linear(hidden_dim, num_actions)\n",
    "        self.action2_head = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "        # Critic head\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.body(x)\n",
    "        \n",
    "        # Actor outputs (logits for probability distributions)\n",
    "        rule_logits = self.rule_head(features)\n",
    "        action1_logits = self.action1_head(features)\n",
    "        action2_logits = self.action2_head(features)\n",
    "        \n",
    "        # Critic output (state value)\n",
    "        value = self.value_head(features)\n",
    "        \n",
    "        return rule_logits, action1_logits, action2_logits, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a689f8",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e89cc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, device=\"cpu\"):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(Experience(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a7ece",
   "metadata": {},
   "source": [
    "### Actor-Critic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a465e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    class Mode(Enum):\n",
    "        TRAIN = 'train'\n",
    "        EVAL = 'eval'\n",
    "\n",
    "    def __init__(self,\n",
    "        state_size,\n",
    "        action_size, # num_actions for action1 and action2\n",
    "        rule_size,\n",
    "        hidden_size,\n",
    "        buffer, # ReplayBuffer instance\n",
    "        gamma=0.99, \n",
    "        lr=1e-3, \n",
    "        entropy_coeff=0.01,\n",
    "        device=None\n",
    "    ):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.rule_size = rule_size\n",
    "        self.gamma = gamma\n",
    "        self.buffer = buffer\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self._mode = self.Mode.TRAIN\n",
    "        \n",
    "        self.network = ActorCriticNetwork(state_size, rule_size, action_size, hidden_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=lr)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @property\n",
    "    def mode(self):\n",
    "        return self._mode\n",
    "\n",
    "    def eval(self):\n",
    "        self._mode = self.Mode.EVAL\n",
    "        self.network.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self._mode = self.Mode.TRAIN\n",
    "        self.network.train()\n",
    "\n",
    "    def act(self, state, deterministic=None):\n",
    "        \"\"\"\n",
    "        Choose actions based on the current state.\n",
    "        deterministic: If True, selects the most likely actions. \n",
    "                       If False, samples from the distribution.\n",
    "                       If None, uses self.mode (deterministic for EVAL).\n",
    "        \"\"\"\n",
    "        if deterministic is None:\n",
    "            deterministic = (self.mode == self.Mode.EVAL)\n",
    "\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rule_logits, a1_logits, a2_logits, _ = self.network(state_tensor)\n",
    "\n",
    "        rule_dist = Categorical(logits=rule_logits)\n",
    "        a1_dist = Categorical(logits=a1_logits)\n",
    "        a2_dist = Categorical(logits=a2_logits)\n",
    "\n",
    "        if deterministic:\n",
    "            rule_action = rule_dist.probs.argmax(dim=-1)\n",
    "            a1_action = a1_dist.probs.argmax(dim=-1)\n",
    "            a2_action = a2_dist.probs.argmax(dim=-1)\n",
    "        else:\n",
    "            rule_action = rule_dist.sample()\n",
    "            a1_action = a1_dist.sample()\n",
    "            a2_action = a2_dist.sample()\n",
    "        \n",
    "        # Save log probabilities of selected actions for training\n",
    "        # (For simplicity, they will be recalculated in learn() based on the batch)\n",
    "        # action_log_probs = rule_dist.log_prob(rule_action) + \\\n",
    "        #                    a1_dist.log_prob(a1_action) + \\\n",
    "        #                    a2_dist.log_prob(a2_action)\n",
    "\n",
    "        # Return actions as a tuple of Python int\n",
    "        return (rule_action.item(), a1_action.item(), a2_action.item())\n",
    "\n",
    "    def learn(self): # Rename step to learn for clarity\n",
    "        if len(self.buffer) < self.buffer.batch_size:\n",
    "            return None # Return None if there's nothing to learn\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample()\n",
    "        # actions has shape (batch_size, 3)\n",
    "\n",
    "        # Get current predictions from the network for states in the batch\n",
    "        rule_logits, a1_logits, a2_logits, state_values = self.network(states)\n",
    "        \n",
    "        # Critic loss\n",
    "        with torch.no_grad():\n",
    "            _, _, _, next_state_values = self.network(next_states)\n",
    "            # If done, then next_state_value = 0\n",
    "            td_targets = rewards + self.gamma * next_state_values * (1 - dones)\n",
    "        \n",
    "        advantage = td_targets - state_values\n",
    "\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        # Actor loss\n",
    "        rule_dist = Categorical(logits=rule_logits)\n",
    "        a1_dist = Categorical(logits=a1_logits)\n",
    "        a2_dist = Categorical(logits=a2_logits)\n",
    "\n",
    "        # Log probabilities of selected actions (from the buffer)\n",
    "        log_probs_rules = rule_dist.log_prob(actions[:, 0])\n",
    "        log_probs_a1 = a1_dist.log_prob(actions[:, 1])\n",
    "        log_probs_a2 = a2_dist.log_prob(actions[:, 2])\n",
    "        \n",
    "        total_log_probs = log_probs_rules + log_probs_a1 + log_probs_a2\n",
    "        \n",
    "        actor_loss = -(total_log_probs * advantage.detach()).mean()\n",
    "        \n",
    "        # Entropy bonus for encouraging exploration\n",
    "        entropy_rules = rule_dist.entropy().mean()\n",
    "        entropy_a1 = a1_dist.entropy().mean()\n",
    "        entropy_a2 = a2_dist.entropy().mean()\n",
    "        total_entropy = entropy_rules + entropy_a1 + entropy_a2\n",
    "        \n",
    "        actor_loss -= self.entropy_coeff * total_entropy\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a058c1",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1295054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_rules = [\n",
    "    InferenceRule.MODUS_PONENS,\n",
    "    InferenceRule.MODUS_TOLLENS,\n",
    "    InferenceRule.AND_INTRODUCTION,\n",
    "    InferenceRule.AND_ELIMINATION1,\n",
    "    InferenceRule.AND_ELIMINATION2,\n",
    "    InferenceRule.DOUBLE_NEGATION_ELIMINATION,\n",
    "    InferenceRule.DOUBLE_NEGATION_INTRODUCTION,\n",
    "    InferenceRule.OR_ELIMINATION,\n",
    "    InferenceRule.HYPOTHETICAL_SYLLOGISM\n",
    "]\n",
    "\n",
    "axioms = {\n",
    "    \"A AND B IMPLIES C\",\n",
    "    \"E OR F IMPLIES B\",\n",
    "    #\"P OR Q IMPLIES F\",\n",
    "    \"A\",\n",
    "    \"E\"\n",
    "}\n",
    "goal = \"C\"\n",
    "\n",
    "env = ProofSearchEnv(axioms=axioms, goal_formula_str=goal, rules=active_rules, max_formulas_in_state=8, max_steps=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b137aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ActorCriticAgent(\n",
    "    env.max_formulas_in_actionable_list,\n",
    "    env.max_formulas_in_actionable_list,\n",
    "    len(active_rules),\n",
    "    hidden_size=72,\n",
    "    buffer=ReplayBuffer(buffer_size=1000, batch_size=64),\n",
    "    gamma=0.9,\n",
    "    entropy_coeff=0.01,\n",
    "    lr=3e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59a6cd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 8,090\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in agent.network.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29edde24",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m next_state, reward, terminated, truncated, info = env.step(action_tuple)\n\u001b[32m     10\u001b[39m agent.buffer.add(state, action_tuple, reward, next_state, terminated)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m state = next_state\n\u001b[32m     13\u001b[39m steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mActorCriticAgent.learn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    129\u001b[39m total_loss = actor_loss + critic_loss\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "agent.train()\n",
    "episodes = 256\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    steps = 0\n",
    "    terminated, truncated = False, False\n",
    "    while not terminated and not truncated:\n",
    "        action_tuple = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action_tuple)\n",
    "        agent.buffer.add(state, action_tuple, reward, next_state, terminated)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    print(f\"ep: {episode+1:>3}/{episodes} | steps: {steps:<3} | {'success' if terminated else 'failure'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c37417",
   "metadata": {},
   "source": [
    "# Results observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axioms {'DBAccess', '(DBAccess OR ContactEstablished) IMPLIES MissionPossible'}\n",
      "\n",
      "env.formulas {'DBAccess', '(DBAccess OR ContactEstablished) IMPLIES MissionPossible', 'DBAccess IMPLIES MissionPossible'}\n",
      "reward 0.8, rule OR_ELIMINATION on [DBAccess] [DBAccess OR ContactEstablished IMPLIES MissionPossible] returns [DBAccess IMPLIES MissionPossible]\n",
      "\n",
      "env.formulas {'DBAccess', '(DBAccess OR ContactEstablished) IMPLIES MissionPossible', 'MissionPossible', 'DBAccess IMPLIES MissionPossible'}\n",
      "reward 64.0, rule MODUS_PONENS on [DBAccess IMPLIES MissionPossible] [DBAccess] returns [MissionPossible]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent.eval()\n",
    "state, _ = env.reset()\n",
    "print(\"axioms\", env.engine.get_known_formulas(), end=\"\\n\\n\")\n",
    "while True:\n",
    "    action_tuple = agent.act(state)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action_tuple)\n",
    "    print(\"env.formulas\", env.engine.get_known_formulas())\n",
    "    print(f\"reward {reward}, rule {info['rule_used']} on [{env.actionable_formulas_list[action_tuple[1]]}] [{env.actionable_formulas_list[action_tuple[2]]}] returns [{info['newly_derived_formula']}]\")\n",
    "    print()\n",
    "    state = next_state\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
